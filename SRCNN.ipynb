{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLLm6pgCn8nl"
      },
      "source": [
        "# SRCNN\n",
        "\n",
        "La superresolución de imágenes (SR) es un desafío fundamental en la visión por computadora, cuyo objetivo es reconstruir imágenes de alta resolución a partir de versiones de baja resolución. Este problema tiene aplicaciones clave en áreas como la medicina, la vigilancia, la fotografía y la restauración de imágenes históricas. Con el auge del Deep Learning, se han logrado avances significativos, permitiendo obtener resultados cada vez más precisos y realistas.\n",
        "\n",
        "En este notebook, exploraremos uno de los enfoques más influyentes basados en redes neuronales profundas: la Super-Resolution Convolutional Neural Network ([SRCNN](https://arxiv.org/abs/1501.00092)). Este modelo aprende de extremo a extremo a transformar imágenes de baja resolución en imágenes de alta calidad, utilizando capas convolucionales para extraer características y reconstruir detalles perdidos, como se ilustra en el siguiente diagrama.\n",
        "\n",
        "![SRCNN Architecture](https://raw.githubusercontent.com/jorge-jrzz/UEA-ML_SRCNN/refs/heads/main/imgs/srcnn-architecture.png)\n",
        "\n",
        "Para entrenar nuestro modelo, utilizaremos el conjunto de datos [Dog and Cat Detection](https://www.kaggle.com/andrewmvd/dog-and-cat-detection) disponible en Kaggle. Descargaremos las imágenes mediante el módulo kagglehub, aunque también puedes obtenerlas manualmente desde el sitio web.\n",
        "\n",
        "## Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/jorge-jrzz/UEA-ML_SRCNN/refs/heads/main/install_datasets.py\n",
        "!wget https://raw.githubusercontent.com/jorge-jrzz/UEA-ML_SRCNN/refs/heads/main/requirements.txt\n",
        "%pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from install_datasets import download_dataset\n",
        "\n",
        "download_dataset(\"andrewmvd/dog-and-cat-detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aseguramos que la ruta donde se va a guardar el modelo en Google Drive existe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "mkdir -p /content/drive/MyDrive/super_resolution\n",
        "cp -r training/* /content/drive/MyDrive/super_resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_PATH = '/content/drive/MyDrive/super_resolution/model.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diz7uSpxoCym"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Model\n",
        "from keras.losses import mse\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import Sequence, plot_model\n",
        "from keras.layers import Input, Conv2D, ReLU\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definición de Parámetros para la Extracción de Parches\n",
        "\n",
        "- **SCALE (2.0)**: Define el factor de escala para la super-resolución. Un valor de 2.0 significa que intentamos mejorar la resolución por un factor de 2.\n",
        "  \n",
        "- **INPUT_DIM (33)**: Tamaño de los parches de entrada que alimentaremos a la red. Estos parches son extraídos de las imágenes de baja resolución.\n",
        "  \n",
        "- **LABEL_SIZE (21)**: Tamaño de los parches objetivo (ground truth). Son más pequeños que los de entrada debido a que la red reduce el tamaño durante el procesamiento.\n",
        "  \n",
        "- **PAD (6)**: Representa el padding necesario para alinear correctamente los parches de entrada y salida. Se calcula como (INPUT_DIM - LABEL_SIZE) / 2.\n",
        "  \n",
        "- **STRIDE (14)**: Determina cuántos píxeles nos desplazamos al extraer parches consecutivos. Un valor menor crea más parches con mayor solapamiento.\n",
        "\n",
        "- **SUBSET_SIZE (500)**: Número de imagenes que utilizaremos para entrenar y validar la red.\n",
        "\n",
        "- **BATCH_SIZE (1024)**: Número de parches que procesamos en cada iteración del entrenamiento, basandose en la memoria disponible.\n",
        "\n",
        "- **EPOCHS (12)**: Número de veces que recorremos el conjunto de datos durante el entrenamiento.\n",
        "\n",
        "- **VALIDATION_SPLIT (0.1)**: Proporción de los datos que utilizaremos para la validación durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SCALE = 2.0             # Factor de escala para la super-resolución\n",
        "INPUT_DIM = 33          # Tamaño de los parches de entrada (33x33 píxeles)\n",
        "LABEL_SIZE = 21         # Tamaño de los parches de salida (21x21 píxeles)\n",
        "PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0)  # Padding necesario (6 píxeles)\n",
        "STRIDE = 14             # Paso para la ventana deslizante (14 píxeles)\n",
        "SUBSET_SIZE = 500       # Tamaño del subconjunto de datos a generar\n",
        "BATCH_SIZE = 1024       # Tamaño del lote para el entrenamiento\n",
        "EPOCHS = 12             # Número de épocas de entrenamiento\n",
        "VALIDATION_SPLIT = 0.1  # 10% de los datos para validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQe32lDcNQiQ"
      },
      "source": [
        "## Preparación de Imágenes para Super-Resolución\n",
        "\n",
        "En el proceso de super-resolución, necesitamos trabajar con pares de imágenes: una versión de baja resolución (entrada) y una versión de alta resolución (objetivo). A continuación, se implementan varias funciones para preparar los datos de entrenamiento.\n",
        "\n",
        "### Función: `reduce_image()`\n",
        "\n",
        "Redimensiona una imagen según un factor de escala dado.\n",
        "\n",
        "**Args**:\n",
        "- image_array: Array NumPy que representa la imagen\n",
        "- factor: Factor de escala para redimensionar (ej. 0.5 para reducir a la mitad, 2.0 para duplicar)\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "Array NumPy con la imagen redimensionada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oBWfViXpkxD"
      },
      "outputs": [],
      "source": [
        "def resize_image(image_array, factor):\n",
        "    # Convertimos el array de NumPy a una imagen PIL\n",
        "    original_image = Image.fromarray(image_array)\n",
        "    \n",
        "    # Calculamos el nuevo tamaño multiplicando las dimensiones originales por el factor\n",
        "    new_size = np.array(original_image.size) * factor\n",
        "    new_size = new_size.astype(np.int32)  # Convertimos a enteros\n",
        "    new_size = tuple(new_size)  # Convertimos a tupla para resize()\n",
        "    \n",
        "    # Redimensionamos la imagen\n",
        "    resized = original_image.resize(new_size)\n",
        "    \n",
        "    # Convertimos de vuelta a array y aseguramos que sea uint8\n",
        "    resized = img_to_array(resized)\n",
        "    resized = resized.astype(np.uint8)\n",
        "    \n",
        "    return resized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CylaFgop3RR"
      },
      "source": [
        "### Función: `downsize_upsize_image()`\n",
        "\n",
        "Simula el proceso de degradación de resolución reduciendo y luego aumentando el tamaño de una imagen. Este proceso es fundamental para generar pares de entrenamiento para super-resolución.\n",
        "\n",
        "**Args**:\n",
        "- image: Imagen original de alta resolución\n",
        "- scale: Factor de escala para la reducción/ampliación\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "Imagen de \"baja resolución\" del mismo tamaño que la original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8AJN4yLp1lL"
      },
      "outputs": [],
      "source": [
        "def downsize_upsize_image(image, scale):\n",
        "    # Primero reducimos la imagen (downsampling)\n",
        "    scaled = resize_image(image, 1.0 / scale)\n",
        "    \n",
        "    # Luego la ampliamos de nuevo al tamaño original (upsampling)\n",
        "    # Esto crea una versión de menor calidad pero mismo tamaño\n",
        "    scaled = resize_image(scaled, scale / 1.0)\n",
        "    \n",
        "    return scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScO1y0IvpxF8"
      },
      "source": [
        "### Función: `tight_crop_image()`\n",
        "\n",
        "Recorta una imagen para asegurar que sus dimensiones sean divisibles por el factor de escala. Esto es necesario para que el proceso de extracción de parches funcione correctamente.\n",
        "\n",
        "**Args**:\n",
        "- image: Imagen a recortar\n",
        "- scale: Factor de escala\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "Imagen recortada con dimensiones divisibles por scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m40Smg8dprVh"
      },
      "outputs": [],
      "source": [
        "def tight_crop_image(image, scale):\n",
        "    height, width = image.shape[:2]\n",
        "    \n",
        "    # Ajustamos el ancho y alto para que sean divisibles por el factor de escala\n",
        "    width -= int(width % scale)\n",
        "    height -= int(height % scale)\n",
        "    \n",
        "    # Devolvemos la imagen recortada\n",
        "    return image[:height, :width]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv86rnGIp_Ri"
      },
      "source": [
        "### Funciones para Extraer Parches de Entrenamiento\n",
        "\n",
        "#### Función: `crop_input()`\n",
        "\n",
        "Extrae un parche de la imagen de entrada en la posición (x,y).\n",
        "    \n",
        "**Args**:\n",
        "- image: Imagen de entrada (baja resolución)\n",
        "- x, y: Coordenadas de la esquina superior izquierda del parche\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "Parche de tamaño INPUT_DIM x INPUT_DIM\n",
        "\n",
        "#### Función:`crop_output()`\n",
        "\n",
        "Extrae un parche de la imagen objetivo (alta resolución) en la posición (x,y), considerando el padding necesario para alinear con la salida de la red.\n",
        "\n",
        "**Args**:\n",
        "- image: Imagen objetivo (alta resolución)\n",
        "- x, y: Coordenadas de la esquina superior izquierda del parche\n",
        "\n",
        "**Returns**:\n",
        "\n",
        "Parche de tamaño LABEL_SIZE x LABEL_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1TTga8Mp9Qr"
      },
      "outputs": [],
      "source": [
        "def crop_input(image, x, y):\n",
        "    y_slice = slice(y, y + INPUT_DIM)\n",
        "    x_slice = slice(x, x + INPUT_DIM)\n",
        "    return image[y_slice, x_slice]\n",
        "\n",
        "def crop_output(image, x, y):\n",
        "    # Añadimos PAD para compensar la reducción de tamaño en la red\n",
        "    y_slice = slice(y + PAD, y + PAD + LABEL_SIZE)\n",
        "    x_slice = slice(x + PAD, x + PAD + LABEL_SIZE)\n",
        "    return image[y_slice, x_slice]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABMLW8A2qPam"
      },
      "source": [
        "### Carga y Preparación del Conjunto de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos las rutas de todas las imágenes en el directorio\n",
        "file_patten = (Path('/content') / 'images' / '*.png')\n",
        "file_pattern = str(file_patten)\n",
        "dataset_paths = [*glob(file_pattern)]\n",
        "\n",
        "# Para reducir el tiempo de entrenamiento, seleccionamos un subconjunto aleatorio\n",
        "dataset_paths = np.random.choice(dataset_paths, SUBSET_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualización de una Imagen de Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h66Ezd_qNpD"
      },
      "outputs": [],
      "source": [
        "# Mostramos una imagen aleatoria del conjunto de datos para verificar\n",
        "path = np.random.choice(dataset_paths)\n",
        "img = plt.imread(path)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generación de Parches para Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I9xFpj6pZLe"
      },
      "outputs": [],
      "source": [
        "# Creamos directorios para almacenar los parches\n",
        "%%bash\n",
        "\n",
        "mkdir -p data\n",
        "mkdir -p training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBWNUT7aquW1",
        "outputId": "fa26015a-110f-4f22-8790-67165617c5e0"
      },
      "outputs": [],
      "source": [
        "# Procesamos cada imagen para generar parches de entrenamiento\n",
        "for image_path in tqdm(dataset_paths):\n",
        "    # Obtenemos el nombre base del archivo\n",
        "    filename = Path(image_path).stem\n",
        "    \n",
        "    # Cargamos la imagen y la convertimos a un array NumPy\n",
        "    image = load_img(image_path)\n",
        "    image = img_to_array(image)\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    # Recortamos la imagen para que sus dimensiones sean divisibles por SCALE\n",
        "    image = tight_crop_image(image, SCALE)\n",
        "    \n",
        "    # Generamos la versión de baja resolución\n",
        "    scaled = downsize_upsize_image(image, SCALE)\n",
        "    \n",
        "    # Obtenemos las dimensiones de la imagen\n",
        "    height, width = image.shape[:2]\n",
        "    \n",
        "    # Extraemos parches deslizando una ventana por la imagen\n",
        "    for y in range(0, height - INPUT_DIM + 1, STRIDE):\n",
        "        for x in range(0, width - INPUT_DIM + 1, STRIDE):\n",
        "            # Extraemos el parche de entrada (baja resolución)\n",
        "            crop = crop_input(scaled, x, y)\n",
        "            \n",
        "            # Extraemos el parche objetivo correspondiente (alta resolución)\n",
        "            target = crop_output(image, x, y)\n",
        "            \n",
        "            # Guardamos los parches como archivos NumPy\n",
        "            np.save(f'data/{filename}_{x}_{y}_input.np', crop)\n",
        "            np.save(f'data/{filename}_{x}_{y}_output.np', target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7IMlM7_s005"
      },
      "source": [
        "### Cargador de Datos para Entrenamiento\n",
        "\n",
        "Como no podemos mantener todos los parches en memoria simultáneamente, los hemos guardado en disco. Ahora necesitamos un cargador de datos que lea estos parches en lotes durante el entrenamiento. Implementaremos esto mediante la clase `PatchesDataset`, que hereda de `Sequence` de Keras para permitir la carga eficiente de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo4CvM8ruL8s"
      },
      "outputs": [],
      "source": [
        "class PatchesDataset(Sequence):\n",
        "    def __init__(self, batch_size, *args, **kwargs):\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Obtenemos las rutas de todos los archivos de entrada y salida\n",
        "        self.input = [*glob('data/*_input.np.npy')]\n",
        "        self.output = [*glob('data/*_output.np.npy')]\n",
        "        \n",
        "        # Ordenamos las listas para asegurar la correspondencia entre entrada y salida\n",
        "        self.input.sort()\n",
        "        self.output.sort()\n",
        "        \n",
        "        # Guardamos el número total de muestras\n",
        "        self.total_data = len(self.input)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.total_data / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Obtiene un lote de datos.\n",
        "        \n",
        "        Args:\n",
        "            idx: Índice del lote\n",
        "        \n",
        "        Returns:\n",
        "            Tupla (entradas, salidas) con los datos del lote\n",
        "        \"\"\"\n",
        "        # Calculamos los índices de inicio y fin para este lote\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = min((idx + 1) * self.batch_size, self.total_data)\n",
        "        \n",
        "        # Inicializamos arrays para almacenar los datos del lote\n",
        "        batch_input = []\n",
        "        batch_output = []\n",
        "        \n",
        "        # Cargamos cada muestra del lote\n",
        "        for i in range(batch_start, batch_end):\n",
        "            # Cargamos el parche de entrada\n",
        "            input_data = np.load(self.input[i])\n",
        "            input_data = input_data.astype(np.float32) / 255.0  # Normalizamos a [0,1]\n",
        "            batch_input.append(input_data)\n",
        "            \n",
        "            # Cargamos el parche de salida correspondiente\n",
        "            output_data = np.load(self.output[i])\n",
        "            output_data = output_data.astype(np.float32) / 255.0  # Normalizamos a [0,1]\n",
        "            batch_output.append(output_data)\n",
        "        \n",
        "        # Convertimos las listas a arrays NumPy\n",
        "        return np.array(batch_input), np.array(batch_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl701L_etihR"
      },
      "source": [
        "Crea una instancia del generador de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJvwcubXyPwE",
        "outputId": "137cc983-0f73-4793-fd62-94cb9b3eef1f"
      },
      "outputs": [],
      "source": [
        "train_ds = PatchesDataset(BATCH_SIZE)\n",
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ZC5fq-t0yJ"
      },
      "source": [
        "Visualizar la forma (dimensiones) de los lotes de entrada y salida:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hdrUlE-xuJo",
        "outputId": "d9ac42cb-fa36-4b3b-d530-ff4110340e0d"
      },
      "outputs": [],
      "source": [
        "input, output = train_ds[0]\n",
        "input.shape, output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "464q7HAZobT-"
      },
      "source": [
        "## Modelo SRCNN\n",
        "\n",
        "La arquitectura de SRCNN es elegante en su simplicidad pero poderosa en sus resultados. El modelo consta de tres capas convolucionales, cada una con un propósito específico en el proceso de super-resolución:\n",
        "\n",
        "1. **Primera capa (Extracción de parches)**: Utiliza 64 filtros con un tamaño de kernel de 9×9 para extraer parches de baja resolución y representarlos como mapas de características de alta dimensión.\n",
        "\n",
        "2. **Segunda capa (Mapeo no lineal)**: Emplea 32 filtros con un tamaño de kernel de 1×1 para mapear los mapas de características de alta dimensión a mapas de características que representan parches de alta resolución.\n",
        "\n",
        "3. **Tercera capa (Reconstrucción)**: Utiliza filtros con un tamaño de kernel de 5×5 para combinar las predicciones de los parches superpuestos, reconstruyendo la imagen de alta resolución final.\n",
        "\n",
        "Esta arquitectura implementa el enfoque de aprendizaje de extremo a extremo para la super-resolución, donde la red aprende directamente la transformación de imágenes de baja resolución a alta resolución.\n",
        "\n",
        "### Parámetros del Modelo SRCNN\n",
        "\n",
        "- **Inicialización de pesos**: Utilizamos la inicialización 'he_normal' (también conocida como inicialización de He), que es especialmente adecuada para capas con activación ReLU. Esta inicialización ayuda a evitar el problema de desvanecimiento del gradiente.\n",
        "\n",
        "- **Función de activación ReLU**: Aplicamos la función de activación Rectified Linear Unit (ReLU) después de las dos primeras capas convolucionales. ReLU introduce no-linealidad en el modelo y ayuda a acelerar la convergencia durante el entrenamiento.\n",
        "\n",
        "- **Tamaños de kernel**:\n",
        "  - Kernel 9×9 en la primera capa: Permite capturar un contexto espacial más amplio para la extracción de características.\n",
        "  - Kernel 1×1 en la segunda capa: Realiza una transformación no lineal punto a punto sin mezclar información espacial.\n",
        "  - Kernel 5×5 en la tercera capa: Proporciona suficiente contexto para reconstruir detalles finos en la imagen final.\n",
        "\n",
        "- **Número de filtros**:\n",
        "  - 64 filtros en la primera capa: Permite extraer un conjunto rico de características de bajo nivel.\n",
        "  - 32 filtros en la segunda capa: Reduce la dimensionalidad mientras mantiene la información relevante.\n",
        "  - Mismo número de filtros que canales de entrada en la capa final: Garantiza que la salida tenga la misma profundidad que la imagen original.\n",
        "\n",
        "#### Función:`crop_output()`\n",
        "\n",
        "Crea un modelo SRCNN (Super-Resolution Convolutional Neural Network).\n",
        "    \n",
        "**Args**:\n",
        "\n",
        "- height: Altura de la imagen de entrada\n",
        "- width: Ancho de la imagen de entrada\n",
        "- depth: Profundidad (canales) de la imagen de entrada\n",
        "    \n",
        "**Returns**:\n",
        "\n",
        "Modelo SRCNN compilado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcDUgtQuoWUm"
      },
      "outputs": [],
      "source": [
        "def create_model(height, width, depth):\n",
        "    # Capa de entrada\n",
        "    input = Input(shape=(height, width, depth))\n",
        "    \n",
        "    # Primera capa convolucional - Extracción de parches\n",
        "    # 64 filtros con kernel 9x9 para extraer características de bajo nivel\n",
        "    x = Conv2D(filters=64, kernel_size=(9, 9), kernel_initializer='he_normal')(input)\n",
        "    x = ReLU()(x)  # Activación ReLU para introducir no-linealidad\n",
        "    \n",
        "    # Segunda capa convolucional - Mapeo no lineal\n",
        "    # 32 filtros con kernel 1x1 para mapear características a representaciones de alta resolución\n",
        "    x = Conv2D(filters=32, kernel_size=(1, 1), kernel_initializer='he_normal')(x)\n",
        "    x = ReLU()(x)  # Activación ReLU\n",
        "    \n",
        "    # Tercera capa convolucional - Reconstrucción\n",
        "    # Filtros con kernel 5x5 para reconstruir la imagen final de alta resolución\n",
        "    output = Conv2D(filters=depth, kernel_size=(5, 5), kernel_initializer='he_normal')(x)\n",
        "    \n",
        "    # Creamos y devolvemos el modelo\n",
        "    return Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función de Pérdida y Optimizador\n",
        "\n",
        "- **Función de pérdida - Error Cuadrático Medio (MSE)**: \n",
        "  El MSE es una elección común para problemas de regresión como la super-resolución. Calcula el promedio de los cuadrados de las diferencias entre los valores predichos y los valores reales. En el contexto de imágenes, mide la diferencia de intensidad de píxeles entre la imagen reconstruida y la imagen objetivo de alta resolución.\n",
        "\n",
        "- **Optimizador - Adam**:\n",
        "  Adam (Adaptive Moment Estimation) es un algoritmo de optimización que combina las ventajas de los algoritmos AdaGrad y RMSProp. Adapta las tasas de aprendizaje para cada parámetro, lo que lo hace eficiente para problemas con gradientes dispersos o ruidosos, como es común en el procesamiento de imágenes.\n",
        "\n",
        "- **Tasa de aprendizaje**:\n",
        "  Utilizamos una tasa de aprendizaje de 0.0003, que es un valor equilibrado para SRCNN. Una tasa demasiado alta podría causar que el entrenamiento sea inestable, mientras que una tasa demasiado baja podría hacer que el entrenamiento sea innecesariamente lento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos el modelo SRCNN\n",
        "model = create_model(INPUT_DIM, INPUT_DIM, 3)  # 3 canales para imágenes RGB\n",
        "\n",
        "# Compilamos el modelo con el optimizador Adam y la función de pérdida de error cuadrático medio\n",
        "model.compile(\n",
        "    optimizer = Adam(learning_rate=1e-3, decay=1e-3 / EPOCHS),  # Tasa de aprendizaje ajustada para SRCNN\n",
        "    loss='mse'  # Error cuadrático medio (Mean Squared Error)\n",
        ")\n",
        "\n",
        "# Mostramos un resumen de la arquitectura del modelo\n",
        "display(model.summary())\n",
        "\n",
        "# Visualizamos la arquitectura del modelo\n",
        "try:\n",
        "    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "    display(Image.open('model.png'))\n",
        "except Exception as e:\n",
        "    print(f\"No se pudo generar la visualización del modelo: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento del Modelo SRCNN\n",
        "\n",
        "El entrenamiento de un modelo de super-resolución como SRCNN requiere una cantidad significativa de datos y tiempo de cómputo. En este notebook, entrenaremos el modelo durante `<EPOCHS>` épocas, lo que debería ser suficiente para observar mejoras significativas en la calidad de las imágenes.\n",
        "\n",
        "Durante el entrenamiento, monitorizaremos la pérdida tanto en el conjunto de entrenamiento como en el de validación. La pérdida de validación nos ayudará a determinar si el modelo está generalizando bien o si está sobreajustando los datos de entrenamiento.\n",
        "\n",
        "Utilizamos el callback `ModelCheckpoint` para guardar automáticamente la mejor versión del modelo según la pérdida de validación. Esto nos asegura que, incluso si el modelo comienza a sobreajustar en épocas posteriores, conservaremos la versión con mejor rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuramos los callbacks para el entrenamiento\n",
        "callbacks = [\n",
        "    # Guardamos el mejor modelo basado en la pérdida de validación\n",
        "    ModelCheckpoint(\n",
        "        filepath='training/srcnn_model_best.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Calculamos el número de muestras para validación\n",
        "total_samples = len(train_ds.input)\n",
        "val_samples = int(total_samples * VALIDATION_SPLIT)\n",
        "train_samples = total_samples - val_samples\n",
        "\n",
        "print(f\"Total de muestras: {total_samples}\")\n",
        "print(f\"Muestras de entrenamiento: {train_samples}\")\n",
        "print(f\"Muestras de validación: {val_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "history = model.fit(\n",
        "    train_ds,  # Generador de datos de entrenamiento\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Guardamos el modelo final\n",
        "model.save(MODEL_PATH)\n",
        "print(\"Modelo guardado correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualización de Resultados del Entrenamiento\n",
        "\n",
        "Para evaluar el progreso del entrenamiento, visualizaremos las curvas de pérdida tanto para el conjunto de entrenamiento como para el de validación. Estas gráficas nos ayudarán a entender:\n",
        "\n",
        "1. Si el modelo está aprendiendo efectivamente (la pérdida disminuye con el tiempo)\n",
        "2. Si el modelo está sobreajustando (la pérdida de validación aumenta mientras la de entrenamiento sigue disminuyendo)\n",
        "3. Si el modelo ha convergido (la pérdida se estabiliza después de cierto número de épocas)\n",
        "\n",
        "Además, visualizaremos algunos ejemplos de super-resolución aplicando nuestro modelo a imágenes de prueba para evaluar cualitativamente su rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizamos las curvas de pérdida\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
        "plt.title('Curvas de Pérdida del Modelo SRCNN')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Error Cuadrático Medio (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1vvjKaTnoRf"
      },
      "source": [
        "## Cargar el modelo desde  Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODljSQEMnqzr"
      },
      "outputs": [],
      "source": [
        "model = load_model(MODEL_PATH, custom_objects={'mse': mse})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zoxz9Tbrbdk"
      },
      "source": [
        "## Evaluación del Modelo SRCNN\n",
        "\n",
        "Una vez entrenado nuestro modelo SRCNN, es fundamental evaluar su rendimiento para comprender qué tan bien realiza la tarea de super-resolución. En esta sección, evaluaremos el modelo de dos maneras:\n",
        "\n",
        "1. **Evaluación cuantitativa**: Utilizaremos métricas objetivas como PSNR (Peak Signal-to-Noise Ratio) y SSIM (Structural Similarity Index) para medir numéricamente la calidad de las imágenes reconstruidas.\n",
        "\n",
        "2. **Evaluación cualitativa**: Visualizaremos ejemplos de imágenes procesadas por nuestro modelo para realizar una evaluación subjetiva de la calidad visual.\n",
        "\n",
        "Estas evaluaciones nos permitirán entender las fortalezas y limitaciones de nuestro modelo SRCNN.\n",
        "\n",
        "### Métricas de Evaluación\n",
        "\n",
        "Para evaluar objetivamente la calidad de las imágenes generadas por nuestro modelo SRCNN, utilizaremos dos métricas principales:\n",
        "\n",
        "- **PSNR (Peak Signal-to-Noise Ratio)**: Mide la relación entre la potencia máxima posible de una señal y la potencia del ruido que afecta a su representación. En el contexto de imágenes, un valor más alto de PSNR generalmente indica una mejor calidad de reconstrucción. Se mide en decibelios (dB).\n",
        "\n",
        "- **SSIM (Structural Similarity Index)**: A diferencia del PSNR, que se basa en el error cuadrático medio, el SSIM considera cambios en la estructura, luminancia y contraste. Produce valores entre -1 y 1, donde 1 indica una similitud perfecta entre las imágenes comparadas.\n",
        "\n",
        "Estas métricas nos ayudarán a cuantificar la mejora que nuestro modelo SRCNN proporciona en comparación con métodos más simples como la interpolación bicúbica.\n",
        "\n",
        "#### Función: `calculate_psnr()`\n",
        "\n",
        "Calcula el Peak Signal-to-Noise Ratio entre dos imágenes.\n",
        "    \n",
        "**Args**:\n",
        "\n",
        "- img1: Primera imagen (referencia)\n",
        "- img2: Segunda imagen (comparación)\n",
        "    \n",
        "**Returns**:\n",
        "\n",
        "Valor PSNR en decibelios (dB)\n",
        "\n",
        "#### Función: `calculate_ssim()`\n",
        "\n",
        "Calcula el Structural Similarity Index entre dos imágenes.\n",
        "\n",
        "Args:\n",
        "- img1: Primera imagen (referencia)\n",
        "- img2: Segunda imagen (comparación)\n",
        "- multichannel: Si es True, considera múltiples canales (RGB)\n",
        "\n",
        "Returns:\n",
        "\n",
        "Valor SSIM entre -1 y 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_psnr(img1, img2):\n",
        "    # Aseguramos que las imágenes estén en el rango [0, 1]\n",
        "    if img1.max() > 1.0:\n",
        "        img1 = img1.astype(np.float32) / 255.0\n",
        "    if img2.max() > 1.0:\n",
        "        img2 = img2.astype(np.float32) / 255.0\n",
        "        \n",
        "    return psnr(img1, img2)\n",
        "\n",
        "def calculate_ssim(img1, img2, multichannel=True):\n",
        "    # Aseguramos que las imágenes estén en el rango [0, 1]\n",
        "    if img1.max() > 1.0:\n",
        "        img1 = img1.astype(np.float32) / 255.0\n",
        "    if img2.max() > 1.0:\n",
        "        img2 = img2.astype(np.float32) / 255.0\n",
        "        \n",
        "    return ssim(img1, img2, multichannel=multichannel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Función para Aplicar Super-Resolución a una Imagen: `apply_srcnn()`\n",
        "\n",
        "Aplica el modelo SRCNN a una imagen para realizar super-resolución.\n",
        "    \n",
        "**Args**:\n",
        "- model: Modelo SRCNN entrenado\n",
        "- img: Imagen de entrada (alta resolución original)\n",
        "- scale: Factor de escala para la super-resolución\n",
        "        \n",
        "**Returns**:\n",
        "\n",
        "Tupla con (imagen original, imagen de baja resolución, imagen reconstruida por SRCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_srcnn(model, img, scale=2.0):\n",
        "    # Aseguramos que la imagen esté en el formato correcto\n",
        "    if isinstance(img, str):\n",
        "        img = load_img(img)\n",
        "        img = img_to_array(img)\n",
        "    \n",
        "    # Recortamos la imagen para que sus dimensiones sean divisibles por scale\n",
        "    img = tight_crop_image(img, scale)\n",
        "    \n",
        "    # Creamos la versión de baja resolución\n",
        "    lr_img = downsize_upsize_image(img, scale)\n",
        "    \n",
        "    # Normalizamos la imagen para la predicción\n",
        "    lr_img_norm = lr_img.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Aplicamos el modelo para obtener la imagen de super-resolución\n",
        "    sr_img = model.predict(np.expand_dims(lr_img_norm, axis=0))[0]\n",
        "    \n",
        "    # Convertimos de vuelta al rango [0, 255]\n",
        "    sr_img = (sr_img * 255.0).astype(np.uint8)\n",
        "    \n",
        "    return img, lr_img, sr_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Función para Visualizar Resultados `visualize_results()`\n",
        "\n",
        "Visualiza los resultados de super-resolución para comparación.\n",
        "    \n",
        "**Args**:\n",
        "- original: Imagen original de alta resolución\n",
        "- bicubic: Imagen de baja resolución (interpolación bicúbica)\n",
        "- srcnn: Imagen reconstruida por SRCNN\n",
        "- title: Título opcional para la figura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGL1WEgerlPP"
      },
      "outputs": [],
      "source": [
        "def visualize_results(original, bicubic, srcnn, title=None):\n",
        "    # Configuramos la figura\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 10))\n",
        "    \n",
        "    # Mostramos las imágenes\n",
        "    axes[0].imshow(original)\n",
        "    axes[0].set_title('Original (Alta Resolución)')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(bicubic)\n",
        "    axes[1].set_title('Bicúbica (Baja Resolución)')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(srcnn)\n",
        "    axes[2].set_title('SRCNN (Reconstruida)')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    # Calculamos y mostramos las métricas\n",
        "    psnr_bicubic = calculate_psnr(original, bicubic)\n",
        "    ssim_bicubic = calculate_ssim(original, bicubic)\n",
        "    \n",
        "    psnr_srcnn = calculate_psnr(original, srcnn)\n",
        "    ssim_srcnn = calculate_ssim(original, srcnn)\n",
        "    \n",
        "    if title:\n",
        "        plt.suptitle(f\"{title}\\n\"\n",
        "                    f\"PSNR: Bicúbica={psnr_bicubic:.2f}dB, SRCNN={psnr_srcnn:.2f}dB | \"\n",
        "                    f\"SSIM: Bicúbica={ssim_bicubic:.4f}, SRCNN={ssim_srcnn:.4f}\", \n",
        "                    fontsize=16)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return psnr_bicubic, ssim_bicubic, psnr_srcnn, ssim_srcnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Edgrd0Ersdg"
      },
      "source": [
        "### Evaluación en Imágenes de Prueba\n",
        "\n",
        "Evaluaremos nuestro modelo SRCNN en algunas imágenes de prueba para ver cómo se desempeña en la tarea de super-resolución. Seleccionaremos algunas imágenes aleatorias de nuestro conjunto de datos y compararemos:\n",
        "\n",
        "1. La imagen original de alta resolución\n",
        "2. La versión de baja resolución obtenida mediante interpolación bicúbica\n",
        "3. La imagen reconstruida por nuestro modelo SRCNN\n",
        "\n",
        "Para cada comparación, calcularemos las métricas PSNR y SSIM para cuantificar la mejora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionamos algunas imágenes aleatorias para evaluar\n",
        "num_test_images = 5\n",
        "test_images = np.random.choice(dataset_paths, num_test_images)\n",
        "\n",
        "# Almacenamos los resultados de las métricas\n",
        "results = {\n",
        "    'psnr_bicubic': [],\n",
        "    'ssim_bicubic': [],\n",
        "    'psnr_srcnn': [],\n",
        "    'ssim_srcnn': []\n",
        "}\n",
        "\n",
        "# Evaluamos cada imagen\n",
        "for i, img_path in enumerate(test_images):\n",
        "    # Aplicamos super-resolución\n",
        "    original, bicubic, srcnn = apply_srcnn(model, img_path)\n",
        "    \n",
        "    # Visualizamos los resultados\n",
        "    img_name = Path(img_path).stem\n",
        "    psnr_bic, ssim_bic, psnr_sr, ssim_sr = visualize_results(\n",
        "        original, bicubic, srcnn, \n",
        "        title=f\"Imagen de prueba {i+1}: {img_name}\"\n",
        "    )\n",
        "    \n",
        "    # Guardamos los resultados\n",
        "    results['psnr_bicubic'].append(psnr_bic)\n",
        "    results['ssim_bicubic'].append(ssim_bic)\n",
        "    results['psnr_srcnn'].append(psnr_sr)\n",
        "    results['ssim_srcnn'].append(ssim_sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculamos los promedios de las métricas\n",
        "avg_psnr_bicubic = np.mean(results['psnr_bicubic'])\n",
        "avg_ssim_bicubic = np.mean(results['ssim_bicubic'])\n",
        "avg_psnr_srcnn = np.mean(results['psnr_srcnn'])\n",
        "avg_ssim_srcnn = np.mean(results['ssim_srcnn'])\n",
        "\n",
        "# Mostramos un resumen de los resultados\n",
        "print(\"Resumen de resultados:\")\n",
        "print(f\"PSNR promedio - Bicúbica: {avg_psnr_bicubic:.2f}dB, SRCNN: {avg_psnr_srcnn:.2f}dB\")\n",
        "print(f\"SSIM promedio - Bicúbica: {avg_ssim_bicubic:.4f}, SRCNN: {avg_ssim_srcnn:.4f}\")\n",
        "print(f\"Mejora PSNR: {avg_psnr_srcnn - avg_psnr_bicubic:.2f}dB\")\n",
        "print(f\"Mejora SSIM: {avg_ssim_srcnn - avg_ssim_bicubic:.4f}\")\n",
        "\n",
        "# Visualizamos los resultados en un gráfico de barras\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico para PSNR\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(['Bicúbica', 'SRCNN'], [avg_psnr_bicubic, avg_psnr_srcnn], color=['blue', 'green'])\n",
        "plt.title('PSNR Promedio')\n",
        "plt.ylabel('PSNR (dB)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Gráfico para SSIM\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['Bicúbica', 'SRCNN'], [avg_ssim_bicubic, avg_ssim_srcnn], color=['blue', 'green'])\n",
        "plt.title('SSIM Promedio')\n",
        "plt.ylabel('SSIM')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de Zonas Específicas\n",
        "\n",
        "Una de las ventajas de los modelos de super-resolución como SRCNN es su capacidad para reconstruir detalles finos en las imágenes. A continuación, analizaremos zonas específicas de algunas imágenes para observar más de cerca cómo el modelo reconstruye estos detalles.\n",
        "\n",
        "Este análisis nos permitirá apreciar mejor las diferencias entre la interpolación bicúbica simple y la reconstrucción mediante SRCNN, especialmente en áreas con texturas complejas, bordes y detalles finos.\n",
        "\n",
        "#### Función `visualize_detail()`\n",
        "Visualiza una región específica de las imágenes con zoom para analizar detalles.\n",
        "    \n",
        "**Args**:\n",
        "- original: Imagen original de alta resolución\n",
        "- bicubic: Imagen de baja resolución (interpolación bicúbica)\n",
        "- srcnn: Imagen reconstruida por SRCNN\n",
        "- region: Tupla (x, y, ancho, alto) que define la región a visualizar. Si es None, se selecciona una región central\n",
        "- zoom: Factor de zoom para la visualización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_detail(original, bicubic, srcnn, region=None, zoom=3):\n",
        "    # Si no se especifica una región, seleccionamos el centro de la imagen\n",
        "    if region is None:\n",
        "        h, w = original.shape[:2]\n",
        "        size = min(h, w) // 4\n",
        "        x = (w - size) // 2\n",
        "        y = (h - size) // 2\n",
        "        region = (x, y, size, size)\n",
        "    \n",
        "    x, y, w, h = region\n",
        "    \n",
        "    # Recortamos las regiones\n",
        "    original_crop = original[y:y+h, x:x+w]\n",
        "    bicubic_crop = bicubic[y:y+h, x:x+w]\n",
        "    srcnn_crop = srcnn[y:y+h, x:x+w]\n",
        "    \n",
        "    # Configuramos la figura\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Primera fila: imágenes completas con rectángulo marcando la región\n",
        "    axes[0, 0].imshow(original)\n",
        "    axes[0, 0].set_title('Original')\n",
        "    axes[0, 0].add_patch(plt.Rectangle((x, y), w, h, edgecolor='red', linewidth=2, fill=False))\n",
        "    \n",
        "    axes[0, 1].imshow(bicubic)\n",
        "    axes[0, 1].set_title('Bicúbica')\n",
        "    axes[0, 1].add_patch(plt.Rectangle((x, y), w, h, edgecolor='red', linewidth=2, fill=False))\n",
        "    \n",
        "    axes[0, 2].imshow(srcnn)\n",
        "    axes[0, 2].set_title('SRCNN')\n",
        "    axes[0, 2].add_patch(plt.Rectangle((x, y), w, h, edgecolor='red', linewidth=2, fill=False))\n",
        "    \n",
        "    # Segunda fila: regiones ampliadas\n",
        "    axes[1, 0].imshow(original_crop)\n",
        "    axes[1, 0].set_title('Original (Detalle)')\n",
        "    \n",
        "    axes[1, 1].imshow(bicubic_crop)\n",
        "    axes[1, 1].set_title('Bicúbica (Detalle)')\n",
        "    \n",
        "    axes[1, 2].imshow(srcnn_crop)\n",
        "    axes[1, 2].set_title('SRCNN (Detalle)')\n",
        "    \n",
        "    # Desactivamos los ejes\n",
        "    for ax in axes.flatten():\n",
        "        ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Seleccionamos una imagen aleatoria para analizar en detalle\n",
        "detail_img_path = np.random.choice(dataset_paths)\n",
        "original, bicubic, srcnn = apply_srcnn(model, detail_img_path)\n",
        "\n",
        "# Visualizamos la imagen completa\n",
        "visualize_results(original, bicubic, srcnn, title=f\"Análisis detallado: {Path(detail_img_path).stem}\")\n",
        "\n",
        "# Visualizamos una región con detalles\n",
        "visualize_detail(original, bicubic, srcnn)\n",
        "\n",
        "# Opcionalmente, podemos seleccionar manualmente una región con detalles interesantes\n",
        "# Por ejemplo, si detectamos una región con textura o bordes complejos\n",
        "h, w = original.shape[:2]\n",
        "# Ejemplo: región en la esquina superior izquierda\n",
        "region_corner = (0, 0, w//4, h//4)\n",
        "visualize_detail(original, bicubic, srcnn, region=region_corner)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Super_Resolution_SRCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
